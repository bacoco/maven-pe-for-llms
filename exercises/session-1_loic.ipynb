{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bacoco/maven-pe-for-llms/blob/main/exercises/session-1_loic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4YXdoce_Dmr"
      },
      "source": [
        "# Session 1 - Prompt Engineering for LLMs Exercises\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/dair-ai/maven-pe-for-llms/blob/main/exercises/session-1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ezUH-LU9_Dmu"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# update or install the necessary libraries\n",
        "!pip install --upgrade openai\n",
        "!pip install --upgrade langchain\n",
        "!pip install --upgrade python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "B3_85k0N_Dmw"
      },
      "outputs": [],
      "source": [
        "# load the libraries\n",
        "import openai\n",
        "import os\n",
        "import IPython\n",
        "from langchain.llms import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# load the environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# API configuration\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQ7vD90I_Dmw"
      },
      "source": [
        "### Using The Chat LLM (GPT-3.5-Turbo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "98R7np4p_Dmw"
      },
      "outputs": [],
      "source": [
        "def get_completion(messages, model=\"gpt-3.5-turbo\", temperature=0, max_tokens=300):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aenzkGLm_Dmx"
      },
      "source": [
        "### Exercise: Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tU9KfrNz_Dmx"
      },
      "source": [
        "Test the prompt below using different temperature values. Try with high and low temperature values, including a temperature value of `0`. Do you see any differences in the outputs?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DFi3CNgK_Dmy",
        "outputId": "7a962196-d7b5-429f-a933-9a22abbe14b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AuthenticationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-dba82f809c1d>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m ]\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-4c90fe498e9c>\u001b[0m in \u001b[0;36mget_completion\u001b[0;34m(messages, model, temperature, max_tokens)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     response = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__prepare_create_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morganization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36m__prepare_create_request\u001b[0;34m(cls, api_key, api_base, api_type, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timeout\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMAX_TIMEOUT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         requestor = api_requestor.APIRequestor(\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mapi_base\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_base\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, key, api_base, api_type, api_version, organization)\u001b[0m\n\u001b[1;32m    136\u001b[0m     ):\n\u001b[1;32m    137\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi_base\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_base\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         self.api_type = (\n\u001b[1;32m    140\u001b[0m             \u001b[0mApiType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/util.py\u001b[0m in \u001b[0;36mdefault_api_key\u001b[0;34m()\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         raise openai.error.AuthenticationError(\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0;34m\"No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         )\n",
            "\u001b[0;31mAuthenticationError\u001b[0m: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details."
          ]
        }
      ],
      "source": [
        "user_message = \"The sky is\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_message\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages, temperature=0.7)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8ptcMc5_Dmz"
      },
      "source": [
        "### Exercise: Explain Like I am 5\n",
        "\n",
        "Modify the prompt below to instruct the model to explain the paragraph in one sentence like \"I am 5\". Do you observe any differences in language style?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jx315EjH_Dmz",
        "outputId": "886f0de1-907b-46ae-d907-007d960e9828"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Antibiotics are medications used to treat bacterial infections by killing or preventing the reproduction of bacteria, but they are ineffective against viral infections and overuse can lead to antibiotic resistance.\n"
          ]
        }
      ],
      "source": [
        "user_message = \"\"\"\n",
        "Antibiotics are a type of medication used to treat bacterial infections. They work by either killing the bacteria or preventing them from reproducing, allowing the body's immune system to fight off the infection. Antibiotics are usually taken orally in the form of pills, capsules, or liquid solutions, or sometimes administered intravenously. They are not effective against viral infections, and using them inappropriately can lead to antibiotic resistance.\n",
        "\n",
        "Explain the above in one sentence:\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_message\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfOd3dMc_Dm0"
      },
      "source": [
        "### Exercise: Unsure About Answer\n",
        "\n",
        "Modify the prompt below to elicit the model to respond that it isn't sure about the answer. Hint: you can try remove important details from the prompt. The goal is to enure that the model doesn't make up an answer if it's not able to find an answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dW1r7m2W_Dm0",
        "outputId": "ba7e57b0-01da-4aab-f88c-07c6769fb03f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mice.\n"
          ]
        }
      ],
      "source": [
        "user_message = \"\"\"\n",
        "Answer the question based on the context below. Keep the answer short and concise. Respond \"Unsure about answer\" if not sure about the answer.\n",
        "\n",
        "Context: Teplizumab traces its roots to a New Jersey drug company called Ortho Pharmaceutical. There, scientists generated an early version of the antibody, dubbed OKT3. Originally sourced from mice, the molecule was able to bind to the surface of T cells and limit their cell-killing potential. In 1986, it was approved to help prevent organ rejection after kidney transplants, making it the first therapeutic antibody allowed for human use.\n",
        "\n",
        "Question: What was OKT3 originally sourced from?\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_message\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLWq60iT_Dm1"
      },
      "source": [
        "### Exercise: Explain Answers\n",
        "\n",
        "Modify the prompt below to instruct the model to provide an explanation for the answer selected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7tb0Q2__Dm1",
        "outputId": "dfc2fa58-d3eb-4864-a0f6-8903ced2b5cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutral\n"
          ]
        }
      ],
      "source": [
        "user_message = \"\"\"\n",
        "Classify the text into neutral, negative or positive.\n",
        "\n",
        "Text: I think the food was okay.\n",
        "\n",
        "Sentiment:\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_message\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2LwJa2I_Dm1"
      },
      "source": [
        "### Exercise: Precise Output and Delimiters\n",
        "Add an additional instruction to use delimiter around the input text. Also, add an instruction to output the label in lowercase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxyDBP3K_Dm2"
      },
      "outputs": [],
      "source": [
        "user_message = \"\"\"\n",
        "Classify the text into neutral, negative or positive.\n",
        "\n",
        "Text: I think the food was okay.\n",
        "\n",
        "Sentiment:\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_message\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSPUVaHh_Dm2"
      },
      "source": [
        "### Exercise: Keep it Short and Concise\n",
        "\n",
        "Modify the prompt below to instruct the model to keep AI responses concise and short."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SkA9zUf_Dm2"
      },
      "outputs": [],
      "source": [
        "user_message = \"\"\"\n",
        "The following is a conversation with an AI research assistant. The assistant tone is technical and scientific.\n",
        "\n",
        "Human: Hello, who are you?\n",
        "AI: Greetings! I am an AI research assistant. How can I help you today?\n",
        "Human: Can you tell me about the creation of black holes?\n",
        "AI:\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_message\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDXByYiB_Dm2"
      },
      "source": [
        "### Exercise: Information Extraction\n",
        "\n",
        "Use the poem below to create a prompt that instructs the model to extract all the verbs, including the number of verbs found."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmXPE1y0_Dm2"
      },
      "outputs": [],
      "source": [
        "user_message = \"\"\"\n",
        "Deep into that darkness peering,\n",
        "\n",
        "Long I stood there, wondering, fearing,\n",
        "\n",
        "Doubting, dreaming dreams no mortals\n",
        "\n",
        "Ever dared to dream before;\n",
        "\n",
        "But the silence was unbroken,\n",
        "\n",
        "And the stillness gave no token,\n",
        "\n",
        "And the only word there spoken\n",
        "\n",
        "Was the whispered word, \"Lenore!\"\n",
        "\n",
        "This I whispered, and an echo\n",
        "\n",
        "Murmured back the word, \"Lenore!\"\n",
        "\n",
        "Merely this, and nothing more.\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_message\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGVQ1Jfy_Dm3"
      },
      "source": [
        "### Exercise: Write Demonstrations\n",
        "\n",
        "Build a few-shot prompt with 5 demonstrations. The task is to classify a text into either positive, negative or neutral. Ensure that the output is only the label (lowercased). Apply a delimiter to the input as well. Try using the format below. You can also use your own format if you prefer.\n",
        "\n",
        "\n",
        "```\n",
        "<instruction>\n",
        "\n",
        "<sentence 1> // <label>\n",
        "<sentence 2> // <label>\n",
        "<sentence 3> // <label>\n",
        "<sentence 4> // <label>\n",
        "<sentence 5> // <label>\n",
        "\n",
        "<sentence to classify>\n",
        "\n",
        "<output indicator>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBMsJQPx_Dm3"
      },
      "outputs": [],
      "source": [
        "# modify the prompt below:\n",
        "\n",
        "user_message = \"\"\"\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_message\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXCgdRw0_Dm3"
      },
      "source": [
        "Apply the previous prompt to the following inputs:\n",
        "\n",
        "```\n",
        "{\"prompt\":\"i feel like i cant think logically at all because i start getting weepy and its so frustrating\", \"label\":\"negative\"}\n",
        "{\"prompt\":\"i was very happy when my scholarship to continue studying at unza was approved after it had been cancelled\",\"label\":\"positive\"}\n",
        "{\"prompt\":\"the rhythmic sound of waves crashing against the shore provided a soothing backdrop to the peaceful coastal town\",\"label\":\"neutral\"}\n",
        "{\"prompt\":\"i was feeling terrified and insane\",\"label\":\"negative\"}\n",
        "{\"prompt\":\"i discover this fabulous feature that left me feeling hopelessly romantic\",\"label\":\"positive\"}\n",
        "{\"prompt\":\"i must say i feel reluctant to have a stranger in my home\",\"label\":\"negative\"}\n",
        "{\"prompt\":\"i still cant help feeling pretty skeptical cuz i didnt do well for my exposure module for sociology oh well will see how it goes\",\"label\":\"negative\"}\n",
        "{\"prompt\":\"the gentle breeze rustled the leaves as the sun cast its warm glow on the tranquil meadow\",\"label\":\"neutral\"}\n",
        "{\"prompt\":\"i feel like im dying here with all the weird things happening in my stomach and you think watching tvs more important\",\"label\":\"negative\"}\n",
        "{\"prompt\":\"i feel very insecure about my safety\",\"label\":\"negative\"}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwfSN7KB_Dm3"
      },
      "outputs": [],
      "source": [
        "# add your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBjdhSs4_Dm3"
      },
      "source": [
        "### Exercise: Step-by-Step Solution\n",
        "\n",
        "Modify the prompt to steer the model to think in steps before providing an answer. Try to be specific about the particular steps you need the model to take."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGm5X3mW_Dm4",
        "outputId": "31eca8ae-29ce-4380-ec35-ebf0163a03a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The sum of the numbers in odd positions is: 4 + 9 + 12 + 1 = 26, which is indeed an even number.\n"
          ]
        }
      ],
      "source": [
        "user_message = \"\"\"\n",
        "Adding all the numbers in odd positions will add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_message\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwHJqa5w_Dm4"
      },
      "source": [
        "## LangChain Exercises (Optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSFy_qxg_Dm4"
      },
      "source": [
        "### Exercise: Convert to LangChain Code\n",
        "\n",
        "The exercise is to convert the following code to LangChain format using OpenAI Chat LLMs. Feel free to use the demo notebooks for help."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyGCBJRH_Dm4"
      },
      "outputs": [],
      "source": [
        "system_message = \"\"\"\n",
        "The following is a conversation with an AI research assistant. The assistant tone is technical and scientific.\n",
        "\"\"\"\n",
        "\n",
        "user_message_1 = \"\"\"\n",
        "Hello, who are you?\n",
        "\"\"\"\n",
        "\n",
        "ai_message_1 = \"\"\"\n",
        "Greeting! I am an AI research assistant. How can I help you today?\n",
        "\"\"\"\n",
        "\n",
        "prompt = \"\"\"\n",
        "Human: Can you tell me about the creation of blackholes?\n",
        "AI:\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": system_message\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_message_1\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": ai_message_1\n",
        "\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N4xBKs5_Dm4"
      },
      "source": [
        "### Exercise: Create a Prompt Template using LangChain\n",
        "\n",
        "Create a prompt template for the following information extraction example. Make sure to convert the code to LangChain format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dTcISI0_Dm4"
      },
      "outputs": [],
      "source": [
        "user_message = \"\"\"\n",
        "Deep into that darkness peering,\n",
        "\n",
        "Long I stood there, wondering, fearing,\n",
        "\n",
        "Doubting, dreaming dreams no mortals\n",
        "\n",
        "Ever dared to dream before;\n",
        "\n",
        "But the silence was unbroken,\n",
        "\n",
        "And the stillness gave no token,\n",
        "\n",
        "And the only word there spoken\n",
        "\n",
        "Was the whispered word, \"Lenore!\"\n",
        "\n",
        "This I whispered, and an echo\n",
        "\n",
        "Murmured back the word, \"Lenore!\"\n",
        "\n",
        "Merely this, and nothing more.\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_message\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2T3Xn95_Dm4"
      },
      "source": [
        "### Exercise: Design Few-Shot Template with LangChain\n",
        "\n",
        "Build a few-shot prompt template with 5 demonstrations using LangChain. The task is to classify a text into either positive, negative or neutral. The output should be lowercased label. Use a delimited around the input as well. Try using the format below. You can also try to tune the prompt template to make it more effective.\n",
        "\n",
        "\n",
        "```\n",
        "<instruction>\n",
        "\n",
        "<sentence 1> // <label>\n",
        "<sentence 2> // <label>\n",
        "<sentence 3> // <label>\n",
        "<sentence 4> // <label>\n",
        "<sentence 5> // <label>\n",
        "\n",
        "<sentence to classify>\n",
        "\n",
        "<output indicator>\n",
        "```\n",
        "\n",
        "Use your few-shot prompt template to classify the following inputs:\n",
        "\n",
        "```\n",
        "{\"prompt\":\"i feel like i cant think logically at all because i start getting weepy and its so frustrating\", \"label\":\"negative\"}\n",
        "{\"prompt\":\"i was very happy when my scholarship to continue studying at unza was approved after it had been cancelled\",\"label\":\"positive\"}\n",
        "{\"prompt\":\"the rhythmic sound of waves crashing against the shore provided a soothing backdrop to the peaceful coastal town\",\"label\":\"neutral\"}\n",
        "{\"prompt\":\"i was feeling terrified and insane\",\"label\":\"negative\"}\n",
        "{\"prompt\":\"i discover this fabulous feature that left me feeling hopelessly romantic\",\"label\":\"positive\"}\n",
        "{\"prompt\":\"i must say i feel reluctant to have a stranger in my home\",\"label\":\"negative\"}\n",
        "{\"prompt\":\"i still cant help feeling pretty skeptical cuz i didnt do well for my exposure module for sociology oh well will see how it goes\",\"label\":\"negative\"}\n",
        "{\"prompt\":\"the gentle breeze rustled the leaves as the sun cast its warm glow on the tranquil meadow\",\"label\":\"neutral\"}\n",
        "{\"prompt\":\"i feel like im dying here with all the weird things happening in my stomach and you think watching tvs more important\",\"label\":\"negative\"}\n",
        "{\"prompt\":\"i feel very insecure about my safety\",\"label\":\"negative\"}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfhvmt_r_Dm5"
      },
      "outputs": [],
      "source": [
        "# add your prompt template below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S52yyOdS_Dm5"
      },
      "source": [
        "### Exercise: Extract Product Mentions and Compose Friendly Reply\n",
        "\n",
        "Add two more instructions to the prompt below:\n",
        "\n",
        "- Extract the product mentioned if any\n",
        "- Output a friendly message acknowledging that you have received the complaint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvoWaTT5_Dm5"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTTVkIzE_Dm5"
      },
      "outputs": [],
      "source": [
        "chat = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mc-glkbC_Dm5",
        "outputId": "3f3789ca-e4f2-46c1-c71f-8f3c59dab026"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "[{\"question\": \"What is the complaint about?\", \"answer\": \"The complaint is about not receiving a pair of shoes that were ordered two weeks ago and the tracking information hasn't been updated in days.\"},\n",
              "{\"question\": \"What is the intensity of the complaint (low, medium or high)?\", \"answer\": \"The intensity of the complaint is medium.\"},\n",
              "{\"question\": \"What is the customer's preferred resolution (if any)?\", \"answer\": \"The customer's preferred resolution is not mentioned in the complaint.\"},\n",
              "{\"question\": \"What is the category of the complaint (e.g., price, quality, shipping, etc)?\", \"answer\": \"The category of the complaint is shipping.\"}]"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "system_message = \"\"\"Your task is to analyze customer complaints and answer questions about the complaint. Output \"NA\" if you are not able to answer the question.\"\"\"\n",
        "\n",
        "prompt = \"\"\"\n",
        "Complaint: {complaint}\n",
        "Questions:\n",
        "1. What is the complaint about?\n",
        "2. What is the intensity of the complaint (low, medium or high)?\n",
        "3. What is the customer's preferred resolution (if any)?\n",
        "4. What is the category of the complaint (e.g., price, quality, shipping, etc)?\n",
        "\n",
        "Answers using this format {output_format}:\n",
        "\"\"\"\n",
        "\n",
        "output_format = \"\"\"[{\"question\": \"answer\"},{\"question\": \"answer\"}] \"\"\"\n",
        "\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=system_message),\n",
        "    HumanMessage(content=prompt.format(complaint = \"I ordered a pair of shoes two weeks ago and still haven't received them. The tracking information hasn't been updated in days and I have no idea where my package is.\", output_format = output_format))\n",
        "]\n",
        "\n",
        "\n",
        "IPython.display.Markdown(chat(messages).content)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchain",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}